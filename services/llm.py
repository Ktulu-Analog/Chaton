"""Service LLM avec support OpenAI uniquement."""
from openai import OpenAI
from typing import Dict, Optional, Callable
import yaml

from config.settings import BASE_URL, API_KEY


class LLMLogger:
    """Logger abstrait pour le service LLM."""

    def __init__(
        self,
        error_callback: Optional[Callable[[str], None]] = None,
        info_callback: Optional[Callable[[str], None]] = None
    ):
        self.error = error_callback or (lambda msg: print(f"ERROR: {msg}"))
        self.info = info_callback or (lambda msg: print(f"INFO: {msg}"))


# Logger global
_llm_logger = LLMLogger()


def set_llm_logger(logger: LLMLogger):
    """Configure le logger pour le service LLM."""
    global _llm_logger
    _llm_logger = logger


def get_llm_client() -> OpenAI:
    """
    Cr√©e et retourne un client OpenAI.

    Returns:
        Instance du client OpenAI
    """
    _llm_logger.info(f"üåê Client OpenAI initialis√© - {BASE_URL}")
    return OpenAI(
        base_url=BASE_URL,
        api_key=API_KEY
    )


def load_capability_rules(path: str = "model_capabilities.yaml") -> dict:
    """
    Charge les r√®gles de d√©tection des capacit√©s depuis un fichier YAML.

    Args:
        path: Chemin vers le fichier de configuration

    Returns:
        Dictionnaire des r√®gles de capacit√©s
    """
    try:
        with open(path, "r", encoding="utf-8") as f:
            return yaml.safe_load(f)["capabilities"]
    except Exception as e:
        _llm_logger.error(f"Erreur chargement r√®gles capacit√©s : {e}")
        return {}


def list_available_models(client: OpenAI) -> Dict[str, Dict[str, bool]]:
    """
    Liste les mod√®les disponibles et d√©tecte leurs capacit√©s.

    Args:
        client: Client OpenAI

    Returns:
        Dictionnaire {model_id: {capability: bool}}
    """
    models = {}

    try:
        rules = load_capability_rules()
    except Exception as e:
        _llm_logger.error(f"Erreur chargement r√®gles capacit√©s : {e}")
        rules = {}

    try:
        response = client.models.list()
    except Exception as e:
        _llm_logger.error(f"Erreur r√©cup√©ration mod√®les : {e}")
        return models

    for m in response.data:
        mid = m.id.lower()
        caps = {}

        # Capacit√©s explicites si pr√©sentes
        capabilities = getattr(m, "capabilities", None)
        if isinstance(capabilities, dict):
            for cap, value in capabilities.items():
                caps[cap.lower()] = bool(value)

        # D√©tection via r√®gles YAML
        for cap, rule in rules.items():
            keywords = rule.get("keywords", [])
            if any(k in mid for k in keywords):
                caps[cap] = True

        if caps:
            models[m.id] = caps

    return models


def get_model_capabilities(client: OpenAI, model_id: str) -> Dict[str, bool]:
    """
    R√©cup√®re les capacit√©s d'un mod√®le sp√©cifique.

    Args:
        client: Client OpenAI
        model_id: ID du mod√®le

    Returns:
        Dictionnaire des capacit√©s du mod√®le
    """
    all_models = list_available_models(client)
    return all_models.get(model_id, {})
