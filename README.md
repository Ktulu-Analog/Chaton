 üêæ Chaton - Assistant IA

Assistant conversationnel intelligent avec support RAG (Retrieval-Augmented Generation), multi-modal et export de documents.

## Fonctionnalit√©s

### LLM Flexible
- **Mode OpenAI** : Connexion √† une API compatible OpenAI (Albert, OpenAI, etc.). C'est son mode de fonctionnement nominal, vous devez poss√©der une cl√© pour l'API Albert de la DiNum.
- **Mode Ollama** : Utilisation de mod√®les locaux via Ollama (GPU fortement recommand√©)
- Support multi-modal : texte, images, code
- Streaming des r√©ponses

### RAG (Retrieval-Augmented Generation)
- **Collections locales** : Via Qdrant (base vectorielle locale)
- **Collections distantes** : Via API Albert
- Recherche avanc√©e
- Reranking avec mod√®le BGE
- Extension automatique du contexte avec chunks adjacents

###  Gestion de documents
- Upload multiple de PDFs
- Extraction de contenu web (URLs)
- Upload d'images pour analyse visuelle
- Support LaTeX dans les r√©ponses

###  Export de documents
- **Word (DOCX)** : Export format√© des conversations
- **Excel (XLSX)** : Export de tableaux et donn√©es structur√©es (version pr√©liminaire alpha)
- **PowerPoint (PPTX)** : G√©n√©ration de pr√©sentations √† partir d'un prompt, d'un document PDF ou du RAG

### Profils int√©gr√©s
- Plusieurs profils (r√¥les) int√©gr√©s via un fichier de configuration des prompts syst√®mes pour s'adapter au contexte ou √† la mission de l'assistant.

###  Interface utilisateur
- Interface Streamlit responsive
- Affichage des sources RAG
- Configuration avanc√©e dans la sidebar

## Pr√©requis

- Python 3.9+ (test√© avec 3.12.8)
- (Optionnel) Qdrant pour les collections locales
- (Optionnel) Ollama pour les mod√®les locaux

##  Installation

### 1. Cr√©er un environnement virtuel

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

### 2. Installer les d√©pendances

**Important** : L'installation d√©pend de votre configuration mat√©rielle.

#### Option A : GPU NVIDIA avec CUDA 11.8 (typiquement pour architecture Pascal, non support√©e par les versions de CUDA > 11.8)

```bash
# Installer les d√©pendances de base
pip install -r requirements.txt

# Installer PyTorch avec CUDA 11.8
pip install -r requirements-cuda11.txt
```

#### Option B : GPU NVIDIA avec CUDA 12.x (architectures post-Pascal)

```bash
# Installer les d√©pendances de base
pip install -r requirements.txt

# Installer PyTorch avec CUDA 12.x
pip install -r requirements-cuda12.txt
```

#### Option C : CPU uniquement (sans GPU, adapt√© au fonctionnement Albert API, sinon bonne chance si vous utilisez des LLM locaux)

```bash
# Installer les d√©pendances de base
pip install -r requirements.txt

# Installer PyTorch CPU
pip install -r requirements-cpu.txt
```

#### V√©rifier l'installation CUDA

```bash
# V√©rifier la version CUDA disponible
python -c "import torch; print(f'CUDA disponible: {torch.cuda.is_available()}'); print(f'Version CUDA: {torch.version.cuda}')"
```

### 3. Configuration

Copiez le fichier d'exemple et configurez vos param√®tres :

```bash
cp .env.example .env
```

√âditez `.env` avec vos param√®tres :
